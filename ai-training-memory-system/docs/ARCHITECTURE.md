# Architecture Documentation

## AI Training Memory System - System Architecture

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Three-Layer Architecture](#three-layer-architecture)
3. [Component Interactions](#component-interactions)
4. [Data Flow](#data-flow)
5. [Memory Management](#memory-management)
6. [Vector Storage](#vector-storage)
7. [Solving Pipeline](#solving-pipeline)
8. [Design Decisions](#design-decisions)
9. [Performance Considerations](#performance-considerations)
10. [Scalability](#scalability)

---

## System Overview

The AI Training Memory System implements a memory-augmented architecture that enables continuous learning through persistent vector storage and semantic retrieval.

### Core Principles

1. **Stateful Learning**: Maintain persistent memory across sessions
2. **Vector-Based Retrieval**: Use embeddings for semantic matching
3. **Hybrid Solving**: Combine rule-based and neural approaches
4. **Performance Tracking**: Comprehensive metrics collection

---

## Three-Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Interface Layer                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   CLI    â”‚  â”‚   Demo   â”‚  â”‚  Visual  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Core Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Document â”‚  â”‚ Embedder â”‚  â”‚  Memory  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Solver Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Rule   â”‚  â”‚  Neural  â”‚  â”‚  Hybrid  â”‚ â”‚
â”‚  â”‚  Based   â”‚  â”‚  Network â”‚  â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer 1: Interface Layer

**Purpose**: User interaction and visualization

**Components**:
- **CLI**: Command-line interface for interactive usage
- **Demo**: Pre-built demonstration scenarios
- **Visualization**: Performance charts and graphs

**Responsibilities**:
- Handle user input
- Display results
- Generate visualizations
- Manage demonstrations

### Layer 2: Core Layer

**Purpose**: Memory management and vector operations

**Components**:
- **SimpleDocument**: Data container class
- **TextEmbedder**: SentenceTransformer wrapper
- **AITrainingMemory**: Main memory system

**Responsibilities**:
- Store documents and embeddings
- Manage vector database
- Perform similarity searches
- Track history and metrics

### Layer 3: Solver Layer

**Purpose**: Problem solving and computation

**Components**:
- **RuleBasedSolver**: Mathematical rule engine
- **NeuralNetworkSolver**: Deep learning model
- **HybridSolver**: Combined approach

**Responsibilities**:
- Extract numbers and operations
- Apply mathematical rules
- Use neural network predictions
- Select optimal solving method

---

## Component Interactions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Query                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AITrainingMemory                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  1. Hash Generation (MD5)                    â”‚ â”‚
â”‚  â”‚     â†“                                         â”‚ â”‚
â”‚  â”‚  2. Duplicate Check                          â”‚ â”‚
â”‚  â”‚     â†“ (if not duplicate)                     â”‚ â”‚
â”‚  â”‚  3. Text â†’ Embedding (TextEmbedder)          â”‚ â”‚
â”‚  â”‚     â†“                                         â”‚ â”‚
â”‚  â”‚  4. Similarity Search (Cosine)               â”‚ â”‚
â”‚  â”‚     â†“ (if low similarity)                    â”‚ â”‚
â”‚  â”‚  5. Hybrid Solver                            â”‚ â”‚
â”‚  â”‚     â”œâ”€ Rule-Based Solver                     â”‚ â”‚
â”‚  â”‚     â””â”€ Neural Network (optional)             â”‚ â”‚
â”‚  â”‚     â†“                                         â”‚ â”‚
â”‚  â”‚  6. Store Result                             â”‚ â”‚
â”‚  â”‚     â†“                                         â”‚ â”‚
â”‚  â”‚  7. Track Performance                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Result                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### 1. Query Processing Flow

```
User Query
    â”‚
    â”œâ”€â†’ Text Normalization
    â”‚       â””â”€â†’ MD5 Hash Generation
    â”‚
    â”œâ”€â†’ Duplicate Check
    â”‚   â”œâ”€â†’ [FOUND] Return from Memory âš¡ (Fast Path)
    â”‚   â””â”€â†’ [NOT FOUND] Continue
    â”‚
    â”œâ”€â†’ Text Embedding (384D)
    â”‚       â””â”€â†’ SentenceTransformer
    â”‚
    â”œâ”€â†’ Similarity Search
    â”‚   â”œâ”€â†’ Cosine Similarity Calculation
    â”‚   â”œâ”€â†’ Threshold Filtering
    â”‚   â””â”€â†’ [HIGH MATCH] Return Similar ðŸ” (Medium Path)
    â”‚
    â”œâ”€â†’ Problem Solving
    â”‚   â”œâ”€â†’ Rule-Based Solver
    â”‚   â”‚   â”œâ”€â†’ Number Extraction (Regex)
    â”‚   â”‚   â”œâ”€â†’ Operation Identification
    â”‚   â”‚   â””â”€â†’ Mathematical Computation
    â”‚   â”‚
    â”‚   â””â”€â†’ Neural Network (if enabled)
    â”‚       â”œâ”€â†’ Embedding Input
    â”‚       â”œâ”€â†’ Forward Pass
    â”‚       â””â”€â†’ Prediction Output
    â”‚
    â”œâ”€â†’ Result Storage
    â”‚   â”œâ”€â†’ Create Document
    â”‚   â”œâ”€â†’ Generate Embedding
    â”‚   â”œâ”€â†’ Store in Memory
    â”‚   â””â”€â†’ Update History
    â”‚
    â””â”€â†’ Performance Tracking
        â”œâ”€â†’ Record Time
        â”œâ”€â†’ Update Metrics
        â””â”€â†’ Log Query
```

### 2. Training Flow

```
Training Examples
    â”‚
    â”œâ”€â†’ For Each Example:
    â”‚   â”œâ”€â†’ Create Document
    â”‚   â”œâ”€â†’ Generate Embedding
    â”‚   â””â”€â†’ Add to Memory
    â”‚
    â”œâ”€â†’ Prepare Training Data
    â”‚   â”œâ”€â†’ Collect Embeddings (X)
    â”‚   â”œâ”€â†’ Collect Solutions (y)
    â”‚   â””â”€â†’ Filter Valid Data
    â”‚
    â”œâ”€â†’ Neural Network Training
    â”‚   â”œâ”€â†’ Split Train/Val
    â”‚   â”œâ”€â†’ Mini-batch Training
    â”‚   â”œâ”€â†’ Loss Calculation
    â”‚   â””â”€â†’ Weight Updates
    â”‚
    â””â”€â†’ Update System
        â”œâ”€â†’ Increment Training Cycles
        â””â”€â†’ Update Hybrid Solver
```

---

## Memory Management

### Storage Structure

```python
AITrainingMemory:
    memory_documents: List[SimpleDocument]
        [0]: Document("What is 5+3?", {solution: 8, ...})
        [1]: Document("Calculate 10-4", {solution: 6, ...})
        [2]: Document("What is 7*6?", {solution: 42, ...})
        ...
    
    memory_embeddings: List[np.ndarray]
        [0]: array([0.12, -0.34, ..., 0.56])  # 384D
        [1]: array([0.23, 0.11, ..., -0.22])  # 384D
        [2]: array([-0.45, 0.67, ..., 0.89])  # 384D
        ...
    
    question_history: defaultdict(list)
        "a3f2b8c..." â†’ [
            {timestamp: ..., solution: 8, index: 0},
            {timestamp: ..., solution: 8, index: 15}
        ]
```

### Memory Limit Enforcement

```python
if len(memory_documents) >= max_memory_entries:
    # FIFO eviction
    memory_documents.pop(0)
    memory_embeddings.pop(0)
```

### Parallel Storage Guarantee

**Critical Invariant**: 
```
len(memory_documents) == len(memory_embeddings)
memory_documents[i] â†” memory_embeddings[i]
```

This 1:1 correspondence is maintained at all times.

---

## Vector Storage

### Embedding Generation

```
Text: "What is 5 plus 3?"
    â†“
[Tokenization]
    â†“
Tokens: ["what", "is", "5", "plus", "3", "?"]
    â†“
[SentenceTransformer Processing]
    â†“
Embedding: [0.123, -0.456, 0.789, ..., 0.321]  # 384 dimensions
```

### Similarity Calculation

```python
# Cosine Similarity Formula:
similarity = dot(vec1, vec2) / (norm(vec1) * norm(vec2))

# Batch Optimization:
# Instead of loop:
for vec in stored_vecs:
    sim = cosine_similarity(query, vec)

# Use vectorized operation:
similarities = dot(query, stored_vecs.T) / norms
```

### Similarity Threshold

```
Threshold = 0.7 (default)

0.9 - 1.0: Nearly identical (high confidence retrieval)
0.7 - 0.9: Similar (moderate confidence retrieval)
0.5 - 0.7: Somewhat related (low confidence)
< 0.5:     Different (compute new solution)
```

---

## Solving Pipeline

### Multi-Phase Approach

#### Phase 1: Instant Retrieval (Duplicate Check)
- **Method**: MD5 hash comparison
- **Speed**: ~0.001s
- **Accuracy**: 100% for exact matches

#### Phase 2: Semantic Retrieval (Similarity Search)
- **Method**: Cosine similarity
- **Speed**: ~0.01s per 1000 vectors
- **Accuracy**: High for similar wordings

#### Phase 3: Rule-Based Solving
- **Method**: Regex + operation rules
- **Speed**: ~0.1s
- **Accuracy**: 95%+ for standard math

#### Phase 4: Neural Network Enhancement
- **Method**: Feedforward NN
- **Speed**: ~0.05s
- **Accuracy**: Improves with training

### Decision Tree

```
Is exact duplicate?
â”œâ”€ YES â†’ Return from memory (Phase 1) âš¡
â””â”€ NO â†’ Check similarity
    â”œâ”€ High (â‰¥0.9) â†’ Return similar (Phase 2) ðŸ”
    â””â”€ Low (<0.9) â†’ Compute solution
        â”œâ”€ Rule-based solve (Phase 3) ðŸ“
        â”œâ”€ Neural network enhance (Phase 4) ðŸ§ 
        â””â”€ Store & return
```

---

## Design Decisions

### 1. Why 384 Dimensions?

**Rationale**: 
- Balance between expressiveness and efficiency
- Standard for `all-MiniLM-L6-v2` model
- Proven effective for semantic similarity
- Fits in memory for large datasets

### 2. Why MD5 Hashing?

**Rationale**:
- Fast computation (microseconds)
- Collision probability negligible for text
- Simple implementation
- Standard library support

### 3. Why Hybrid Approach?

**Rationale**:
- Rule-based: Reliable, explainable, fast
- Neural network: Learns patterns, handles edge cases
- Combination: Best of both worlds

### 4. Why In-Memory Storage?

**Rationale**:
- Fast access (no disk I/O)
- Simple implementation
- Sufficient for typical use cases
- Save/load for persistence

### 5. Why Cosine Similarity?

**Rationale**:
- Scale-invariant
- Normalized to [0, 1]
- Standard for embedding comparison
- Efficient batch computation

---

## Performance Considerations

### Time Complexity

| Operation | Complexity | Notes |
|-----------|-----------|-------|
| Add to memory | O(d) | d=384, embedding generation |
| Duplicate check | O(1) | Hash table lookup |
| Similarity search | O(nÂ·d) | n=documents, d=384 |
| Rule-based solve | O(m) | m=text length |
| NN forward pass | O(dÂ·h) | h=hidden layer sizes |

### Space Complexity

| Component | Space | Notes |
|-----------|-------|-------|
| Document | ~1KB | Text + metadata |
| Embedding | 3KB | 384 Ã— 8 bytes |
| History entry | ~100B | Timestamp + refs |
| **Per memory** | **~4KB** | Total per entry |
| **1000 memories** | **~4MB** | Typical usage |

### Optimization Strategies

1. **Batch Operations**: Process multiple embeddings together
2. **Numpy Vectorization**: Use numpy for similarity calculations
3. **Lazy Loading**: Load neural network only if needed
4. **Memory Limits**: Cap total entries to prevent bloat
5. **FIFO Eviction**: Remove oldest entries when full

---

## Scalability

### Horizontal Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User 1    â”‚ â†’ Memory Instance 1
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User 2    â”‚ â†’ Memory Instance 2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User 3    â”‚ â†’ Memory Instance 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each user gets isolated memory instance.

### Vertical Scaling

- **Small**: 1K memories, ~4MB, single-threaded
- **Medium**: 10K memories, ~40MB, batch processing
- **Large**: 100K memories, ~400MB, distributed search

### Database Integration (Future)

```
In-Memory Cache
    â†“ (miss)
Vector Database (e.g., Pinecone, Weaviate)
    â†“ (miss)
Traditional Database (PostgreSQL, MongoDB)
```

---

## Security Considerations

1. **Input Validation**: Sanitize all user inputs
2. **Resource Limits**: Enforce memory caps
3. **No Code Execution**: Pure mathematical operations
4. **Safe Serialization**: Use pickle with caution
5. **Error Handling**: Graceful degradation

---

## Future Enhancements

1. **Distributed Storage**: Support for remote vector databases
2. **Batch Inference**: GPU acceleration for neural network
3. **Advanced Retrieval**: HNSW or other approximate search
4. **Multi-Modal**: Support images, code, etc.
5. **Active Learning**: Identify and request labels for uncertain cases

---

**Architecture Version**: 1.0.0  
**Last Updated**: 2026-02-06  
**Maintained By**: AI Training Memory System Team
